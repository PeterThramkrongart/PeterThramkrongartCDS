---
title: "Preprocessing and topic-modeling horror literature from Project Gutenberg"
author: "Peter Thramkrongart"
output: github_document
---

```{r setup}
#Loading packages

pacman::p_load(
  tidyverse,
  tm,
  tidytext,
  topicmodels,
  reshape2,
  LDAvis,
  servr,
  NLP,
  gutenbergr,
  text2vec,
  textstem,
  openNLP,
  tictoc, 
  spacyr,
  tokenizers,
  openNLPmodels.en,
  udpipe
)

load("workspace.RData")

#install.packages("openNLPmodels.en", dependencies=TRUE, repos = "http://datacube.wu.ac.at/")

#spacy_install()
```


```{r}
#making a lists of the horror titles that we are going to use
IDs <-
  gutenberg_metadata %>%
  filter(str_detect(gutenberg_bookshelf, "Horror") == T &
           language == "en") %>% distinct(title, .keep_all = T)

IDs %>% glimpse()

```

```{r}

#Downloading the data

texts <- gutenberg_download(IDs$gutenberg_id, meta_fields = c("title", "author"))

#collapsing data into single texts

texts <- texts %>% group_by(title) %>% mutate(text = glue::glue_collapse(text, " ")) %>% unique()


#removing unnecessary characters and 

texts$text <-
  texts$text %>% str_replace_all("[^[:alnum:][:space:]'\\.,]", "")

texts %>% write_csv("data/texts.csv")

texts <- read_csv("data/texts.csv")
texts %>% glimpse()



tic()
chunks <-
  chunk_text(
    texts$text,
    chunk_size = 100,
    doc_id = texts$title,
    lowercase = F,
    strip_punct = F
  )
toc()#32.97 sec elapsed

text_tibble <-
  tibble(
    "title" = names(chunks) %>% str_replace_all("-\\d+", ""),
    "titleChunked" = names(chunks),
    "text" = chunks
  )


```




```{r}

#defining a generic function for creating and fitting LDA models

custom_LDA_func <- function(df, n) {
  tokens = df$text %>% word_tokenizer()
  it = itoken(tokens, ids = df$title, progressbar = T)
  v = create_vocabulary(it, stopwords = stopwords::stopwords())
  v = prune_vocabulary(v,
                       term_count_min = 5,
                       doc_proportion_max = 0.2)
  
  vectorizer = vocab_vectorizer(v)
  dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
  
  lda_model = LDA$new(
    n_topics = n,
    doc_topic_prior = 0.1,
    topic_word_prior = 0.01
  )
  doc_topic_distr =
    lda_model$fit_transform(
      x = dtm,
      n_iter = 1000,
      convergence_tol = 0.001,
      n_check_convergence = 25,
      progressbar = T
    )
  return(lda_model)
}

```

```{r}

raw_LDA <- custom_LDA_func(texts, 6)

raw_LDA %>% write_rds("data/raw_LDA.rds")

raw_LDA <- read_rds("data/raw_LDA.rds")

raw_LDA$plot()


chunked_LDA <- custom_LDA_func(text_tibble, 20)

chunked_LDA %>% write_rds("data/chunked_LDA.rds")

chunked_LDA <- read_rds("chunked_LDA.rds")

chunked_LDA$plot()



tic()
lemmatized_tibble <- text_tibble %>% mutate(text = text %>% lemmatize_strings())
toc()#897.18 sec elapsed

lemmatized_tibble$text <- lemmatized_tibble$text %>% tolower()

lemmatized_tibble %>% write_csv("data/lemmatized_tibble.csv")

lemmatized_tibble <- read_csv("data/lemmatized_tibble.csv")

lemmatized_tibble %>% glimpse()

#fitting model on lemmatized texts
lemmatized_LDA <- custom_LDA_func(lemmatized_tibble, 6)

lemmatized_LDA %>% write_rds("data/lemmatized_LDA.rds")

lemmatized_LDA <- read_rds("data/lemmatized_LDA.rds")
#this part is not visible in the knitted document. It starts a server that hosts the visualization.
lemmatized_LDA$plot()

```


```{r}
spacy_initialize()

spacy_initialize(condaenv = "C:/Users/thram_000/anaconda3/envs/spacy_condaenv",save_profile = TRUE)

spacey_test<- spacy_parse(unlist(chunks[1:2]))

spacey_test %>% glimpse()


tic()

spacy_lemmas <- spacy_parse(unlist(chunks))

toc()#1778.81 sec elapsed


spacy_lemmas %>% write_csv("data/spacy_lemmas.csv")


spacy_lemmas <- read_csv("data/spacy_lemmas.csv")

tic()
spacy_texts <- spacy_lemmas %>% filter(pos == "ADJ" |
                                    pos == "ADV" |
                                    pos == "NOUN" |
                                    pos == "VERB") %>%
  mutate(title = doc_id %>% str_replace_all("-\\d+", ""),
         titleChunked = doc_id) %>% select(c("lemma", "title", "titleChunked")) %>%
  group_by(titleChunked) %>%
  mutate(lemma = glue::glue_collapse(lemma, " ")) %>%
  unique() %>%
  rename(text = lemma)
toc()#775.12 sec elapsed


spacy_texts %>% write_csv("data/spacy_texts.csv")

spacy_texts <- read.csv("data/spacy_texts.csv")

spacy_texts %>% glimpse()



spacy_LDA <- spacy_texts %>% custom_LDA_func(20)

spacy_LDA %>% saveRDS("data/spacy_LDA.rds")

spacy_LDA <- read_rds("data/spacy_LDA.rds")

spacy_LDA$plot()



```

```{r}

#difining pipeline for POS tagging

sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()


tic()
openNLP_texts <- NULL

for (i in seq_along(text_tibble$titleChunked)) {

  para_text <- text_tibble$text[i] #grapping text
  
  text_s <- as.String(para_text) # tunring into the right string format
  
  #start annotating string
  annotated_string <- annotate(text_s,
                               list(
                                 sent_token_annotator,
                                 word_token_annotator,
                                 pos_tag_annotator
                               ))
  
  
  word_pos <- subset(annotated_string, type == "word") #grapping position of words
  
  tags_v <- sapply(word_pos$features, `[[`, "POS") #grapping pos tags
  
  words_v <- text_s[word_pos] #grapping pos tags
  
  word_pos_df <- data.frame(Token = words_v,
                            POS = tags_v,
                            stringsAsFactors = FALSE) #assembling in dataframe
  
  filtered_df <-
    #we are only interested in nouns verbs and adjectives
    filter(word_pos_df,
           POS == "NN"| POS == "NNS" | str_detect(POS, "VB") | str_detect(POS, "JJ")| str_detect(POS, "RB")) %>%
    select(Token) %>%
    mutate(Token = tolower(Token))
  
  text_v = paste(filtered_df$Token, collapse = " ")
    
  df = tibble(title = text_tibble$title[i],
    titleChunked = text_tibble$titleChunked[i],
              text = text_v)
    openNLP_texts <- rbind(openNLP_texts, df) #adding to the dataframe

  cat("chunk", i,"out of",length(text_tibble$titleChunked), "\r")#status text
}
toc()#6815.61 sec elapsed
tic()
openNLP_texts$text <- openNLP_texts$text %>% lemmatize_strings()
toc()

openNLP_texts %>%  write_csv("data/openNLP_texts.csv")

openNLP_texts <-  read_csv("data/openNLP_texts.csv")


openNLP_texts %>% glimpse()
```

```{r}


#running LDA


openNLP_LDA <- openNLP_texts %>% custom_LDA_func(20)

openNLP_LDA %>% saveRDS("data/openNLP_LDA.rds")

openNLP_LDA <- read_rds("data/openNLP_LDA.rds")

openNLP_LDA$plot()


save.image("workspace.RData")
load("workspace.RData")

#session info
sessioninfo::session_info()
```
